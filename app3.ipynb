{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1afb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, TypedDict, Literal\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# LangGraph & LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "try:\n",
    "    from langchain_community.llms import Ollama\n",
    "    HAVE_OLLAMA = True\n",
    "except Exception:\n",
    "    HAVE_OLLAMA = False\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28520fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    plan: str\n",
    "    route: Literal[\"web_search\", \"math_solver\", \"rag\", \"direct\"]\n",
    "    scratch: List[str]\n",
    "    tool_result: Any\n",
    "    final_answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e9fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_embedder():\n",
    "    return SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def load_docs_from_dir(docs_dir: str) -> List[Any]:\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(docs_dir):\n",
    "        for fn in files:\n",
    "            path = os.path.join(root, fn)\n",
    "            ext = fn.lower().split(\".\")[-1]\n",
    "            try:\n",
    "                if ext == \"pdf\":\n",
    "                    docs.extend(PyPDFLoader(path).load())\n",
    "                elif ext in (\"docx\",):\n",
    "                    docs.extend(Docx2txtLoader(path).load())\n",
    "                elif ext in (\"txt\", \"md\"):\n",
    "                    docs.extend(TextLoader(path, encoding=\"utf-8\").load())\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {path}: {e}\")\n",
    "    return docs\n",
    "\n",
    "def build_vectorstore(chunks, persist_dir: str):\n",
    "    embeddings = default_embedder()\n",
    "    vs = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=persist_dir)\n",
    "    vs.persist()\n",
    "    return vs\n",
    "\n",
    "def ensure_vectorstore(persist_dir: str):\n",
    "    embeddings = default_embedder()\n",
    "    return Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef7d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Syeda Aiman Mumtaz\\AppData\\Local\\Temp\\ipykernel_14312\\3907045616.py:48: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  '''if use_ollama and HAVE_OLLAMA:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Please set OPENROUTER_API_KEY in your .env file\")\n",
    "\n",
    "def tool_web_search(query: str, k: int = 5) -> List[Dict[str, str]]:\n",
    "    results = []\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            for r in ddgs.text(query, max_results=k):\n",
    "                results.append({\"title\": r.get(\"title\", \"\"), \"href\": r.get(\"href\", \"\"), \"body\": r.get(\"body\", \"\")})\n",
    "    except Exception as e:\n",
    "        results.append({\"title\": \"search_error\", \"href\": \"\", \"body\": str(e)})\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Helper function to extract numbers ---\n",
    "def extract_number(text: str):\n",
    "    \"\"\"Extract last numerical value from text (float or int).\"\"\"\n",
    "    matches = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n",
    "    if matches:\n",
    "        try:\n",
    "            return float(matches[-1]) if \".\" in matches[-1] else int(matches[-1])\n",
    "        except:\n",
    "            return matches[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Calculator Tool ---\n",
    "def tool_calculator(expr: str) -> dict:\n",
    "    \"\"\"Enhanced calculator with structured output.\"\"\"\n",
    "    try:\n",
    "        # Standardize expression\n",
    "        expr = expr.replace(\"^\", \"**\")\n",
    "\n",
    "        # Allowed sympy functions\n",
    "        allowed = {k: getattr(sp, k) for k in [\n",
    "            \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\",\n",
    "            \"log\", \"ln\", \"exp\", \"sqrt\", \"factorial\",\n",
    "            \"ceiling\", \"floor\", \"Abs\", \"diff\", \"integrate\"\n",
    "        ]}\n",
    "\n",
    "        # Constants\n",
    "        allowed.update({\n",
    "            \"pi\": sp.pi,\n",
    "            \"e\": sp.E,\n",
    "            \"inf\": sp.oo,\n",
    "            \"golden_ratio\": (1 + sp.sqrt(5)) / 2\n",
    "        })\n",
    "\n",
    "        # Parse and evaluate\n",
    "        val = sp.sympify(expr, locals=allowed)\n",
    "        numeric_val = sp.N(val)\n",
    "\n",
    "        return {\n",
    "            \"reasoning\": f\"Expression `{expr}` evaluated successfully.\",\n",
    "            \"final_answer\": float(numeric_val)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"reasoning\": f\"Calculator error: {e}\",\n",
    "            \"final_answer\": None\n",
    "        }\n",
    "\n",
    "\n",
    "# --- GSM8k Math Solver ---\n",
    "def tool_gsm8k_math_solve(problem: str, use_ollama: bool = True, model: str = \"mistral\") -> dict:\n",
    "    \"\"\"Math Problem Solver with structured output.\"\"\"\n",
    "    reasoning = \"\"\n",
    "    final_answer = None\n",
    "\n",
    "    # --- Try DeepSeek via OpenRouter ---\n",
    "    if OPENROUTER_API_KEY:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": \"deepseek/deepseek-r1-0528-qwen3-8b:free\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a math tutor. Solve step by step with reasoning.\"},\n",
    "                        {\"role\": \"user\", \"content\": problem}\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                reasoning = content\n",
    "                final_answer = extract_number(content)\n",
    "            else:\n",
    "                reasoning = f\"DeepSeek API error {response.status_code}: {response.text}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            reasoning = f\"DeepSeek error: {e}\"\n",
    "\n",
    "    # --- Fallback Solver (simple heuristics) ---\n",
    "    if not final_answer:\n",
    "        numbers = list(map(float, re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", problem)))\n",
    "        problem_lower = problem.lower()\n",
    "\n",
    "        if len(numbers) >= 2:\n",
    "            operations = {\n",
    "                (\"sum\", \"total\", \"add\", \"plus\"): sum(numbers),\n",
    "                (\"difference\", \"subtract\", \"minus\"): numbers[0] - sum(numbers[1:]),\n",
    "                (\"product\", \"multiply\", \"times\"): np.prod(numbers),\n",
    "                (\"divide\", \"ratio\", \"per\"): numbers[0] / numbers[1] if len(numbers) == 2 else None,\n",
    "                (\"average\", \"mean\"): sum(numbers) / len(numbers)\n",
    "            }\n",
    "\n",
    "            for keywords, operation in operations.items():\n",
    "                if any(word in problem_lower for word in keywords) and operation is not None:\n",
    "                    reasoning = f\"Heuristic detected operation `{keywords[0]}`.\"\n",
    "                    final_answer = operation\n",
    "                    break\n",
    "\n",
    "    # --- Return structured result ---\n",
    "    return {\n",
    "        \"reasoning\": reasoning if reasoning else \"No detailed reasoning available.\",\n",
    "        \"final_answer\": final_answer\n",
    "    }\n",
    "\n",
    "def tool_rag_query(question: str, persist_dir: str, k: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced RAG with better document retrieval\"\"\"\n",
    "    try:\n",
    "        vs = ensure_vectorstore(persist_dir)\n",
    "        # Use semantic search with reranking\n",
    "        docs = vs.similarity_search(question, k=k)\n",
    "        \n",
    "        # Extract and format relevant information\n",
    "        contexts = []\n",
    "        metadata = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            # Clean and format the context\n",
    "            context = doc.page_content.strip()\n",
    "            contexts.append(context)\n",
    "            \n",
    "            # Enhance metadata with source information\n",
    "            meta = doc.metadata.copy()\n",
    "            meta[\"relevance_score\"] = doc.metadata.get(\"score\", 1.0)\n",
    "            metadata.append(meta)\n",
    "        \n",
    "        return {\n",
    "            \"contexts\": contexts,\n",
    "            \"docs\": metadata,\n",
    "            \"summary\": \"\\n\".join(f\"Document {i+1}:\\n{ctx}\" for i, ctx in enumerate(contexts[:2]))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"RAG not available: {e}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "768215d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Make sure to get the API key from environment variables\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Please set OPENROUTER_API_KEY in your .env file\")\n",
    "\n",
    "def query_deepseek(user_query: str) -> str:\n",
    "    \"\"\"Query the DeepSeek model through OpenRouter API\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"HTTP-Referer\": \"http://localhost:7860\",\n",
    "                \"X-Title\": \"LangGraph Agentic Controller\"\n",
    "            },\n",
    "            json={\n",
    "                \"model\": \"deepseek/deepseek-r1-0528-qwen3-8b:free\",  # Using the free model\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are a routing agent. Analyze the query and decide which tool should handle it.\n",
    "                        Choose from: math_solver, web_search, rag, or direct.\n",
    "                        - math_solver: for calculations, math problems, equations\n",
    "                        - web_search: for real-world facts, current events, people\n",
    "                        - rag: for document-based questions\n",
    "                        - direct: for simple queries that need no tools\n",
    "                        Respond ONLY with one of these four options.\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": user_query\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.json()[\"choices\"][0][\"message\"][\"content\"].strip().lower()\n",
    "            print(f\"Raw response from model: {content}\")  # Added debugging\n",
    "            # Ensure we get one of our valid routes\n",
    "            valid_routes = {\"math_solver\", \"web_search\", \"rag\", \"direct\"}\n",
    "            if content in valid_routes:\n",
    "                return content\n",
    "            # If model returns something else, map it to the closest valid route\n",
    "            for route in valid_routes:\n",
    "                if route in content:\n",
    "                    return route\n",
    "            return \"direct\"  # fallback\n",
    "        else:\n",
    "            print(f\"OpenRouter API error: {response.status_code}\")\n",
    "            print(f\"Error message: {response.text}\")\n",
    "            return \"direct\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying DeepSeek: {e}\")\n",
    "        return \"direct\"\n",
    "\n",
    "def controller_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Route queries using DeepSeek through OpenRouter\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    try:\n",
    "        route = query_deepseek(query)\n",
    "        state[\"route\"] = route\n",
    "        state[\"plan\"] = f\"Plan: Using {route} to handle the query.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Router error: {e}\")\n",
    "        state[\"route\"] = \"direct\"\n",
    "        state[\"plan\"] = \"Plan: Fallback to direct response due to routing error.\"\n",
    "    return state\n",
    "\n",
    "def web_search_node(state: AgentState) -> AgentState:\n",
    "    return {**state, \"scratch\": [\"web_search done\"], \"tool_result\": tool_web_search(state[\"query\"])}\n",
    "\n",
    "def math_solver_node(state: AgentState) -> AgentState:\n",
    "    query = state[\"query\"].strip()\n",
    "    # Strict math expression detection\n",
    "    if re.fullmatch(r\"[0-9\\.\\s\\+\\-\\*\\/\\^\\(\\)]+\", query):\n",
    "        result = tool_calculator(query)\n",
    "        return {**state, \"scratch\": [\"calculator used\"], \"tool_result\": result}\n",
    "    else:\n",
    "        # Send word problems (like GSM8K) to DeepSeek solver\n",
    "        result = tool_gsm8k_math_solve(query)\n",
    "        return {**state, \"scratch\": [\"gsm8k solver used\"], \"tool_result\": result}\n",
    "\n",
    "def rag_node(state: AgentState, persist_dir: str) -> AgentState:\n",
    "    return {**state, \"scratch\": [\"rag used\"], \"tool_result\": tool_rag_query(state[\"query\"], persist_dir)}\n",
    "\n",
    "def synthesizer_node(state: AgentState) -> AgentState:\n",
    "    if state[\"route\"] == \"web_search\":\n",
    "        items = state[\"tool_result\"] or []\n",
    "        answer = \"\\n\".join([f\"{i['title']}: {i['body']}\" for i in items[:3]]) or \"No results.\"\n",
    "    elif state[\"route\"] == \"math_solver\":\n",
    "        answer = f\"Result: {state['tool_result']}\"\n",
    "    elif state[\"route\"] == \"rag\":\n",
    "        ctxs = state[\"tool_result\"].get(\"contexts\", [])\n",
    "        answer = \"\\n---\\n\".join(ctxs[:2]) or \"No context found.\"\n",
    "    else:\n",
    "        answer = \"General knowledge route chosen.\"\n",
    "    return {**state, \"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9b57461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OpenRouter connection...\n",
      "API Key present: Yes\n",
      "API Key length: 73\n",
      "\n",
      "Response status code: 200\n",
      "\n",
      "Raw response from DeepSeek: direct\n",
      "Raw response from model: math_solver\n",
      "Final determined route: 'math_solver'\n",
      "\n",
      "Test successful! The routing system is working.\n"
     ]
    }
   ],
   "source": [
    "# Test OpenRouter connection\n",
    "test_query = \"What is 2+2?\"\n",
    "print(\"Testing OpenRouter connection...\")\n",
    "print(f\"API Key present: {'Yes' if OPENROUTER_API_KEY else 'No'}\")\n",
    "print(f\"API Key length: {len(OPENROUTER_API_KEY) if OPENROUTER_API_KEY else 0}\")\n",
    "\n",
    "try:\n",
    "    # Make the API request with detailed debugging\n",
    "    response = requests.post(\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"http://localhost:7860\",\n",
    "            \"X-Title\": \"LangGraph Agentic Controller\"\n",
    "        },\n",
    "        json={\n",
    "            \"model\": \"deepseek/deepseek-r1-0528-qwen3-8b:free\",  # Using the free model\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a routing agent. Choose from: math_solver, web_search, rag, or direct.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": test_query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResponse status code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        content = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        print(f\"\\nRaw response from DeepSeek: {content}\")\n",
    "        \n",
    "        route = query_deepseek(test_query)  # Test the actual routing function\n",
    "        print(f\"Final determined route: '{route}'\")\n",
    "        \n",
    "        print(\"\\nTest successful! The routing system is working.\")\n",
    "    else:\n",
    "        print(f\"\\nError response from OpenRouter:\")\n",
    "        print(response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError connecting to OpenRouter: {str(e)}\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(\"1. Check if your .env file exists and contains OPENROUTER_API_KEY\")\n",
    "    print(\"2. Verify your API key is valid at https://openrouter.ai/keys\")\n",
    "    print(\"3. Make sure you have internet connectivity\")\n",
    "    print(\"\\nFull error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ff6516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Framework\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BenchmarkEvaluator:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "    def evaluate_lama(self, examples: List[Dict[str, str]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate on LAMA benchmark\n",
    "        examples: List of dicts with 'question' and 'answer' keys\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        correct = 0\n",
    "        \n",
    "        for ex in tqdm(examples, desc=\"Evaluating LAMA\"):\n",
    "            result = self.pipeline({\"query\": ex[\"question\"]})\n",
    "            predicted = result[\"final_answer\"].lower()\n",
    "            actual = ex[\"answer\"].lower()\n",
    "            \n",
    "            # Check if actual answer is in predicted text\n",
    "            is_correct = actual in predicted\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "                \n",
    "            results.append({\n",
    "                \"question\": ex[\"question\"],\n",
    "                \"predicted\": predicted,\n",
    "                \"actual\": actual,\n",
    "                \"correct\": is_correct\n",
    "            })\n",
    "        \n",
    "        accuracy = correct / len(examples)\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"results\": results,\n",
    "            \"total_examples\": len(examples)\n",
    "        }\n",
    "    \n",
    "    def evaluate_gsm8k(self, examples: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate on GSM8k benchmark\n",
    "        examples: List of dicts with 'question' and 'answer' keys\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        correct = 0\n",
    "        \n",
    "        for ex in tqdm(examples, desc=\"Evaluating GSM8k\"):\n",
    "            result = self.pipeline({\"query\": ex[\"question\"]})\n",
    "            predicted = None\n",
    "            actual = float(ex[\"answer\"])  # Convert answer to float first\n",
    "            is_correct = False\n",
    "            \n",
    "            try:\n",
    "                # Extract numerical answer from the result\n",
    "                predicted_text = result[\"final_answer\"]\n",
    "                # Look for \"Result:\" or \"Answer:\" in the text\n",
    "                for prefix in [\"Result:\", \"Answer:\", \"Numerical Answer:\"]:\n",
    "                    if prefix in predicted_text:\n",
    "                        number_text = predicted_text.split(prefix)[-1].split()[0]\n",
    "                        predicted = float(''.join(\n",
    "                            filter(lambda x: x.isdigit() or x in '.-', number_text)))\n",
    "                        break\n",
    "                        \n",
    "                if predicted is not None:\n",
    "                    # Check if the answer is within 1% relative error\n",
    "                    is_correct = abs(predicted - actual) / abs(actual) < 0.01\n",
    "                    if is_correct:\n",
    "                        correct += 1\n",
    "                        \n",
    "            except Exception:\n",
    "                pass  # Keep default values for failed parsing\n",
    "                \n",
    "            results.append({\n",
    "                \"question\": ex[\"question\"],\n",
    "                \"predicted\": predicted,\n",
    "                \"actual\": actual,\n",
    "                \"correct\": is_correct\n",
    "            })\n",
    "        \n",
    "        accuracy = correct / len(examples)\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"results\": results,\n",
    "            \"total_examples\": len(examples)\n",
    "        }\n",
    "\n",
    "# Example test sets\n",
    "lama_examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"What is the chemical symbol for gold?\", \"answer\": \"Au\"}\n",
    "]\n",
    "\n",
    "gsm8k_examples = [\n",
    "    {\n",
    "        \"question\": \"Janet has 3 apples. She buys 5 more. How many apples does she have now?\",\n",
    "        \"answer\": \"8\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A train travels 120 kilometers in 2 hours. What is its speed in kilometers per hour?\",\n",
    "        \"answer\": \"60\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d26bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(persist_dir: Optional[str]):\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"controller\", controller_node)\n",
    "    graph.add_node(\"web_search\", web_search_node)\n",
    "    graph.add_node(\"math_solver\", math_solver_node)\n",
    "    graph.add_node(\"rag\", lambda s: rag_node(s, persist_dir))\n",
    "    graph.add_node(\"synth\", synthesizer_node)\n",
    "\n",
    "    graph.set_entry_point(\"controller\")\n",
    "    graph.add_conditional_edges(\"controller\", lambda s: s[\"route\"], {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"math_solver\": \"math_solver\",\n",
    "        \"rag\": \"rag\",\n",
    "        \"direct\": \"synth\"\n",
    "    })\n",
    "    graph.add_edge(\"web_search\", \"synth\")\n",
    "    graph.add_edge(\"math_solver\", \"synth\")\n",
    "    graph.add_edge(\"rag\", \"synth\")\n",
    "    graph.add_edge(\"synth\", END)\n",
    "    return graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c52f722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, app):\n",
    "        self.app = app\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.app.invoke({\n",
    "            **{\n",
    "                \"plan\": \"\",\n",
    "                \"route\": \"direct\",\n",
    "                \"scratch\": [],\n",
    "                \"tool_result\": None,\n",
    "                \"final_answer\": \"\"\n",
    "            },\n",
    "            **state\n",
    "        })\n",
    "\n",
    "import gradio as gr\n",
    "import socket\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "\n",
    "def find_free_port(start_port=7860, max_port=7960):\n",
    "    \"\"\"Find a free port in the given range.\"\"\"\n",
    "    for port in range(start_port, max_port + 1):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('', port))\n",
    "                return port\n",
    "            except OSError:\n",
    "                continue\n",
    "    raise OSError(f\"No free ports found in range {start_port}-{max_port}\")\n",
    "\n",
    "def detect_file_type(file_path, original_filename):\n",
    "    \"\"\"Detect file type using file extension and basic content checking\"\"\"\n",
    "    # First try by extension\n",
    "    ext = os.path.splitext(original_filename)[1].lower()\n",
    "    if ext in ['.pdf', '.docx', '.doc', '.txt', '.md']:\n",
    "        return ext\n",
    "    \n",
    "    # If no extension or unrecognized, try mime type\n",
    "    mime_type, _ = mimetypes.guess_type(original_filename)\n",
    "    if mime_type:\n",
    "        ext_mapping = {\n",
    "            'application/pdf': '.pdf',\n",
    "            'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',\n",
    "            'application/msword': '.doc',\n",
    "            'text/plain': '.txt',\n",
    "            'text/markdown': '.md'\n",
    "        }\n",
    "        if mime_type in ext_mapping:\n",
    "            return ext_mapping[mime_type]\n",
    "    \n",
    "    # Try content-based detection (basic)\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            header = f.read(8)  # Read first 8 bytes\n",
    "            \n",
    "            # PDF signature check\n",
    "            if header.startswith(b'%PDF'):\n",
    "                return '.pdf'\n",
    "            \n",
    "            # DOC/DOCX check (basic)\n",
    "            if header.startswith(b'PK\\x03\\x04'):  # DOCX is a ZIP file\n",
    "                return '.docx'\n",
    "            \n",
    "            # For text files, try to decode as UTF-8\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as tf:\n",
    "                    tf.read(1024)  # Try reading some content\n",
    "                return '.txt'  # If we can read it as text, assume it's a text file\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return '.txt'  # Default to text if we can't determine type\n",
    "\n",
    "def process_uploaded_file(file_obj, persist_dir):\n",
    "    \"\"\"Process an uploaded file and add it to the RAG store\"\"\"\n",
    "    if file_obj is None:\n",
    "        return \"No file uploaded\"\n",
    "    \n",
    "    temp_path = None\n",
    "    try:\n",
    "        # Handle file upload from Gradio\n",
    "        if hasattr(file_obj, 'name'):\n",
    "            # Regular file object\n",
    "            original_filename = os.path.basename(file_obj.name)\n",
    "            file_content = file_obj.read()\n",
    "        else:\n",
    "            # Bytes object from Gradio\n",
    "            original_filename = \"uploaded_document\"\n",
    "            file_content = file_obj\n",
    "        \n",
    "        # Create temp file with no extension first\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode='wb') as temp_file:\n",
    "            if isinstance(file_content, bytes):\n",
    "                temp_file.write(file_content)\n",
    "            else:\n",
    "                temp_file.write(file_content.encode('utf-8'))\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        # Detect file type\n",
    "        file_ext = detect_file_type(temp_path, original_filename)\n",
    "        \n",
    "        # Create a new temp file with correct extension\n",
    "        new_temp_path = temp_path + file_ext\n",
    "        os.rename(temp_path, new_temp_path)\n",
    "        temp_path = new_temp_path\n",
    "        \n",
    "        # Load the document based on file type\n",
    "        try:\n",
    "            if file_ext == '.pdf':\n",
    "                docs = PyPDFLoader(temp_path).load()\n",
    "            elif file_ext in ['.docx', '.doc']:\n",
    "                docs = Docx2txtLoader(temp_path).load()\n",
    "            else:  # .txt or .md\n",
    "                # Try different encodings\n",
    "                encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "                docs = None\n",
    "                for encoding in encodings:\n",
    "                    try:\n",
    "                        docs = TextLoader(temp_path, encoding=encoding).load()\n",
    "                        break\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "                \n",
    "                if docs is None:\n",
    "                    return f\"Unable to decode file with any supported encoding\"\n",
    "            \n",
    "            # Process documents\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            chunks = text_splitter.split_documents(docs)\n",
    "            \n",
    "            # Build or update vectorstore\n",
    "            vs = build_vectorstore(chunks, persist_dir)\n",
    "            \n",
    "            return f\"Successfully processed {original_filename} ({len(chunks)} chunks created)\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_details = traceback.format_exc()\n",
    "            return f\"Error processing file content: {str(e)}\\nDetails:\\n{error_details}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        return f\"Error handling file: {str(e)}\\nDetails:\\n{error_details}\"\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temp file\n",
    "        if temp_path and os.path.exists(temp_path):\n",
    "            try:\n",
    "                os.unlink(temp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def run_gradio(persist_dir=\"./rag_store\"):\n",
    "    # Ensure RAG directory exists\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    \n",
    "    app = build_graph(persist_dir)\n",
    "    pipe = Pipeline(app)\n",
    "\n",
    "    def chat_fn(msg, history):\n",
    "        result = pipe({\"query\": msg})\n",
    "        ans = f\"**Route:** {result['route']}\\n\\n**Plan:** {result['plan']}\\n\\n### Answer\\n{result['final_answer']}\"\n",
    "        return history + [[msg, ans]]\n",
    "\n",
    "    # Find an available port\n",
    "    try:\n",
    "        port = find_free_port()\n",
    "        print(f\"Starting Gradio server on port {port}\")\n",
    "        \n",
    "        with gr.Blocks() as demo:\n",
    "            gr.Markdown(\"# 🤖 LangGraph Agentic Controller (Notebook Version)\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    chatbot = gr.Chatbot(height=500)\n",
    "                    msg = gr.Textbox(label=\"Your query\")\n",
    "                    \n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### Document Upload\")\n",
    "                    file_output = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "                    upload_button = gr.File(\n",
    "                        label=\"Upload Document\",\n",
    "                        file_types=[\".pdf\", \".docx\", \".doc\", \".txt\", \".md\"],\n",
    "                        type=\"binary\"\n",
    "                    )\n",
    "                    \n",
    "            # Set up event handlers\n",
    "            msg.submit(chat_fn, [msg, chatbot], chatbot)\n",
    "            upload_button.change(\n",
    "                fn=lambda f: process_uploaded_file(f, persist_dir),\n",
    "                inputs=[upload_button],\n",
    "                outputs=[file_output]\n",
    "            )\n",
    "            \n",
    "        demo.launch(server_name=\"0.0.0.0\", server_port=port, share=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Gradio server: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "976154ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradio server on port 7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Syeda Aiman Mumtaz\\AppData\\Local\\Temp\\ipykernel_14312\\1134720969.py:185: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/26 17:48:21 [W] [service.go:132] login to server failed: read tcp 10.1.163.176:51256->44.237.78.176:7000: wsarecv: An existing connection was forcibly closed by the remote host.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: math_solver\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio app\n",
    "try:\n",
    "    run_gradio()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to start Gradio app: {e}\")\n",
    "    print(\"Try closing any other Gradio apps or Jupyter notebooks that might be using the ports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8967996d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmarks...\n",
      "\n",
      "Evaluating LAMA benchmark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LAMA:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LAMA:  33%|███▎      | 1/3 [00:07<00:15,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: direct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LAMA:  67%|██████▋   | 2/3 [00:13<00:06,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: direct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LAMA: 100%|██████████| 3/3 [00:18<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: direct\n",
      "LAMA Accuracy: 0.00%\n",
      "\n",
      "Sample LAMA Results:\n",
      "Q: What is the capital of France?\n",
      "A (predicted): general knowledge route chosen.\n",
      "A (actual): paris\n",
      "Correct: False\n",
      "\n",
      "Q: Who wrote Romeo and Juliet?\n",
      "A (predicted): general knowledge route chosen.\n",
      "A (actual): william shakespeare\n",
      "Correct: False\n",
      "\n",
      "Q: What is the chemical symbol for gold?\n",
      "A (predicted): general knowledge route chosen.\n",
      "A (actual): au\n",
      "Correct: False\n",
      "\n",
      "\n",
      "Evaluating GSM8k benchmark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GSM8k:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: math_solver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GSM8k:  50%|█████     | 1/2 [00:17<00:17, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response from model: math_solver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating GSM8k: 100%|██████████| 2/2 [00:38<00:00, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8k Accuracy: 0.00%\n",
      "\n",
      "Sample GSM8k Results:\n",
      "Q: Janet has 3 apples. She buys 5 more. How many apples does she have now?\n",
      "A (predicted): None\n",
      "A (actual): 8.0\n",
      "Correct: False\n",
      "\n",
      "Q: A train travels 120 kilometers in 2 hours. What is its speed in kilometers per hour?\n",
      "A (predicted): None\n",
      "A (actual): 60.0\n",
      "Correct: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarks before launching the app\n",
    "print(\"Running benchmarks...\")\n",
    "\n",
    "# Initialize pipeline and evaluator\n",
    "app = build_graph(\"./rag_store\")\n",
    "pipe = Pipeline(app)\n",
    "evaluator = BenchmarkEvaluator(pipe)\n",
    "\n",
    "# Run LAMA benchmark\n",
    "print(\"\\nEvaluating LAMA benchmark:\")\n",
    "lama_results = evaluator.evaluate_lama(lama_examples)\n",
    "print(f\"LAMA Accuracy: {lama_results['accuracy']:.2%}\")\n",
    "print(\"\\nSample LAMA Results:\")\n",
    "for r in lama_results['results'][:3]:  # Show first 3 examples\n",
    "    print(f\"Q: {r['question']}\")\n",
    "    print(f\"A (predicted): {r['predicted']}\")\n",
    "    print(f\"A (actual): {r['actual']}\")\n",
    "    print(f\"Correct: {r['correct']}\\n\")\n",
    "\n",
    "# Run GSM8k benchmark\n",
    "print(\"\\nEvaluating GSM8k benchmark:\")\n",
    "gsm8k_results = evaluator.evaluate_gsm8k(gsm8k_examples)\n",
    "print(f\"GSM8k Accuracy: {gsm8k_results['accuracy']:.2%}\")\n",
    "print(\"\\nSample GSM8k Results:\")\n",
    "for r in gsm8k_results['results'][:3]:  # Show first 3 examples\n",
    "    print(f\"Q: {r['question']}\")\n",
    "    print(f\"A (predicted): {r['predicted']}\")\n",
    "    print(f\"A (actual): {r['actual']}\")\n",
    "    print(f\"Correct: {r['correct']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
